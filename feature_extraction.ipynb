{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary libraries\n",
    "%pip install pandas\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\joanc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\joanc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\joanc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joanc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Machine Learning\n",
    "import joblib\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Data Loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we load the training and test datasets along with their corresponding gold standard similarity scores from various corpora. We begin by installing and importing the necessary libraries and packages. Next, we define a function to load and process the datasets, and we transform them into structured DataFrames for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 2234\n",
      "Number of test samples: 3108\n"
     ]
    }
   ],
   "source": [
    "from scripts.load_dataset import load_dataset\n",
    "\n",
    "data_dir = 'datasets'\n",
    "\n",
    "# Load Training Data\n",
    "train_data = load_dataset(data_dir, dataset_category='train')\n",
    "columns = ['sentence_0', 'sentence_1', 'score', 'dataset_name']\n",
    "train_data = pd.DataFrame(train_data, columns=columns)\n",
    "\n",
    "# Load Test Data\n",
    "test_data = load_dataset(data_dir, dataset_category='test')\n",
    "columns = ['sentence_0', 'sentence_1', 'score', 'dataset_name']\n",
    "test_data = pd.DataFrame(test_data, columns=columns)\n",
    "\n",
    "# Display the number of samples in each dataset\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is loaded, we apply preprocessing steps to prepare the datasets for feature extraction. Specifically, we:\n",
    "\n",
    "1. Tokenize Sentences: Use NLTK's `word_tokenize` to split sentences into individual tokens.\n",
    "2. Remove Punctuation: Eliminate punctuation tokens to retain only meaningful words.\n",
    "3. Replace Contractions: Replace contractions based on a custom dictionary, inspired by TakenLab's paper. \n",
    "4. Lemmatize Words: Apply NLTK's `WordNetLemmatizer` with Part-of-Speech (POS) tagging to reduce words to their base forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tokens_0'] = train_data['sentence_0'].apply(nltk.word_tokenize)\n",
    "train_data['tokens_1'] = train_data['sentence_1'].apply(nltk.word_tokenize)\n",
    "\n",
    "test_data['tokens_0'] = test_data['sentence_0'].apply(nltk.word_tokenize)\n",
    "test_data['tokens_1'] = test_data['sentence_1'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tokens_0'] = train_data['tokens_0'].apply(lambda tokens: [word for word in tokens if any(char.isalnum() for char in word)])\n",
    "train_data['tokens_1'] = train_data['tokens_1'].apply(lambda tokens: [word for word in tokens if any(char.isalnum() for char in word)])\n",
    "\n",
    "test_data['tokens_0'] = test_data['tokens_0'].apply(lambda tokens: [word for word in tokens if any(char.isalnum() for char in word)])\n",
    "test_data['tokens_1'] = test_data['tokens_1'].apply(lambda tokens: [word for word in tokens if any(char.isalnum() for char in word)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocessing import replace_contractions\n",
    "\n",
    "train_data['tokens_0'] = train_data['tokens_0'].apply(replace_contractions)\n",
    "train_data['tokens_1'] = train_data['tokens_1'].apply(replace_contractions)\n",
    "\n",
    "test_data['tokens_0'] = test_data['tokens_0'].apply(replace_contractions)\n",
    "test_data['tokens_1'] = test_data['tokens_1'].apply(replace_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.preprocessing import lemmatize\n",
    "\n",
    "# Obtain Lemmatized Words\n",
    "train_data['lemmas_0'] = train_data.apply(lambda row: lemmatize(row, \"tokens_0\", True), axis=1)\n",
    "train_data['lemmas_1'] = train_data.apply(lambda row: lemmatize(row, \"tokens_1\", True), axis=1)\n",
    "\n",
    "test_data['lemmas_0'] = test_data.apply(lambda row: lemmatize(row, \"tokens_0\", True), axis=1)\n",
    "test_data['lemmas_1'] = test_data.apply(lambda row: lemmatize(row, \"tokens_1\", True), axis=1)\n",
    "\n",
    "# Join Lemmatized Words\n",
    "train_data['sentence_lemmas_0'] = train_data.apply(lambda row: \" \".join(row[\"lemmas_0\"]), axis=1)\n",
    "train_data['sentence_lemmas_1'] = train_data.apply(lambda row: \" \".join(row[\"lemmas_1\"]), axis=1)\n",
    "\n",
    "test_data['sentence_lemmas_0'] = test_data.apply(lambda row: \" \".join(row[\"lemmas_0\"]), axis=1)\n",
    "test_data['sentence_lemmas_1'] = test_data.apply(lambda row: \" \".join(row[\"lemmas_1\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Load Preprocessed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Training and Test Data\n",
    "train_data.to_csv('datasets/train_preprocessed.csv')\n",
    "test_data.to_csv('datasets/test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training and Test Data\n",
    "train_data = pd.read_csv('datasets/train_preprocessed.csv')\n",
    "test_data = pd.read_csv('datasets/test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Feature Computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.features import harmonic_mean, P_WN\n",
    "\n",
    "def compute_features(data):\n",
    "    \"\"\"\n",
    "    Computes a comprehensive set of similarity features for each pair of sentences in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing 'sentence_lemmas_0' and 'sentence_lemmas_1' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with computed similarity features.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame()\n",
    "\n",
    "    # Similarity Features\n",
    "    features['longest_common_substring'] = data.apply(lambda row: longest_common_substring(row[\"sentence_lemmas_0\"], row[\"sentence_lemmas_1\"]), axis=1)\n",
    "    features['longest_common_subsequence'] = data.apply(lambda row: longest_common_subsequence(row[\"sentence_lemmas_0\"], row[\"sentence_lemmas_1\"]), axis=1)\n",
    "    features['greedy_string_tiling'] = data.apply(lambda row: optimized_gst(row[\"sentence_lemmas_0\"], row[\"sentence_lemmas_1\"], min_match_length=1), axis=1)\n",
    "\n",
    "    # Character n-gram Similarity Features\n",
    "    features['2_gram_char'] = data.apply(lambda row: similarity_char_ngrams(row[\"lemmas_0\"], row[\"lemmas_1\"], 2), axis=1)\n",
    "    features['3_gram_char'] = data.apply(lambda row: similarity_char_ngrams(row[\"lemmas_0\"], row[\"lemmas_1\"], 3), axis=1)\n",
    "    features['4_gram_char'] = data.apply(lambda row: similarity_char_ngrams(row[\"lemmas_0\"], row[\"lemmas_1\"], 4), axis=1)\n",
    "\n",
    "    # Word n-gram Jaccard Similarity Features\n",
    "    features['1_gram_word_Jaccard'] = data.apply(lambda row: similarity_words_ngrams_jaccard(row[\"lemmas_0\"], row[\"lemmas_1\"], 1), axis=1)\n",
    "    features['3_gram_word_Jaccard'] = data.apply(lambda row: similarity_words_ngrams_jaccard(row[\"lemmas_0\"], row[\"lemmas_1\"], 3), axis=1)\n",
    "    features['4_gram_word_Jaccard'] = data.apply(lambda row: similarity_words_ngrams_jaccard(row[\"lemmas_0\"], row[\"lemmas_1\"], 4), axis=1)\n",
    "\n",
    "    # Word n-gram Jaccard Similarity Features without Stopwords\n",
    "    features['2_gram_word_Jaccard_without_SW'] = data.apply(lambda row: similarity_words_ngrams_jaccard(row[\"lemmas_0\"], row[\"lemmas_1\"], 2, use_stopwords=True), axis=1)\n",
    "    features['4_gram_word_Jaccard_without_SW'] = data.apply(lambda row: similarity_words_ngrams_jaccard(row[\"lemmas_0\"], row[\"lemmas_1\"], 4, use_stopwords=True), axis=1)\n",
    "\n",
    "    # Word n-gram Containment Similarity Features without Stopwords (a)\n",
    "    features['1_gram_word_Containment_without_SW_a'] = data.apply(lambda row: similarity_words_ngrams_containment(row[\"lemmas_0\"], row[\"lemmas_1\"], 1, use_stopwords=True), axis=1)\n",
    "    features['2_gram_word_Containment_without_SW_a'] = data.apply(lambda row: similarity_words_ngrams_containment(row[\"lemmas_0\"], row[\"lemmas_1\"], 2, use_stopwords=True), axis=1)\n",
    "\n",
    "    # Word n-gram Containment Similarity Features without Stopwords (b)\n",
    "    features['1_gram_word_Containment_without_SW_b'] = data.apply(lambda row: similarity_words_ngrams_containment(row[\"lemmas_1\"], row[\"lemmas_0\"], 1, use_stopwords=True), axis=1)\n",
    "    features['2_gram_word_Containment_without_SW_b'] = data.apply(lambda row: similarity_words_ngrams_containment(row[\"lemmas_1\"], row[\"lemmas_0\"], 2, use_stopwords=True), axis=1)\n",
    "\n",
    "    features['average_similarity'] = data.apply(lambda row: average_similarity(row[\"lemmas_0\"], row[\"lemmas_1\"]), axis=1)\n",
    "\n",
    "    # Lexical Substitution System Feature\n",
    "    data['lemmas_with_disambiguation_0'] = data.apply(lambda row: tokens_to_synsets_name(row[\"lemmas_0\"]), axis=1)\n",
    "    data['lemmas_with_disambiguation_1'] = data.apply(lambda row: tokens_to_synsets_name(row[\"lemmas_1\"]), axis=1)\n",
    "    features['lexical_substitution_system'] = data.apply(lambda row: similarity_lemmas(row['lemmas_with_disambiguation_0'], row['lemmas_with_disambiguation_1']), axis=1)\n",
    "\n",
    "\n",
    "    # WordNet-Augmented Word Overlap (TakeLab)\n",
    "    features['wordnet_augmented_overlap'] = data.apply(lambda row: harmonic_mean(\n",
    "                P_WN([w for w in row[\"lemmas_0\"] if w not in stopwords], [w for w in row[\"lemmas_1\"] if w not in stopwords]),\n",
    "                P_WN([w for w in row[\"lemmas_1\"] if w not in stopwords], [w for w in row[\"lemmas_0\"] if w not in stopwords])\n",
    "            ), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = compute_features(train_data)\n",
    "features_test = compute_features(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Training Features to a CSV File\n",
    "features_train.to_csv('features/features_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Test Features to a CSV File\n",
    "features_test.to_csv('features/features_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training and Test Features\n",
    "features_train = pd.read_csv('features/features_train.csv')\n",
    "features_test  = pd.read_csv('features/features_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scripts.features import harmonic_mean, P_WN\n",
    "\n",
    "# Add the new feature using the wordnet_augmented_word_overlap function\n",
    "features_test['wordnet_augmented_overlap'] = test_data.apply(lambda row: harmonic_mean(\n",
    "                P_WN([w for w in row[\"lemmas_0\"] if w not in stopwords], [w for w in row[\"lemmas_1\"] if w not in stopwords]),\n",
    "                P_WN([w for w in row[\"lemmas_1\"] if w not in stopwords], [w for w in row[\"lemmas_0\"] if w not in stopwords])\n",
    "            ), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def P_WN(S1, S2):\n",
    "    \"\"\"\n",
    "    Compute P_WN(S1, S2) metric as described in the TakeLab paper.\n",
    "\n",
    "    Parameters:\n",
    "        S1 (list): List of tokenized words from the first sentence.\n",
    "        S2 (list): List of tokenized words from the second sentence.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed P_WN(S1, S2) value.\n",
    "    \"\"\"\n",
    "    if len(S1) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "    S2_set = set(S2) # Slight optimization for membership checks\n",
    "    for word1 in S1:\n",
    "        if word1 in S2_set:\n",
    "            score += 1.0\n",
    "        else:\n",
    "            # Find the best similarity if exact match is not found\n",
    "            best_sim = max((wordnet_path_similarity(word1, word2) for word2 in S2), default=0.0)\n",
    "            score += best_sim\n",
    "\n",
    "    return score / len(S1)\n",
    "\n",
    "def wordnet_path_similarity(word1, word2):\n",
    "    \"\"\"\n",
    "    Compute the maximum WordNet path similarity between all synset pairs of two given words.\n",
    "    Only consider synsets that share the same part-of-speech (POS).\n",
    "\n",
    "    Parameters:\n",
    "        word1 (str): First word.\n",
    "        word2 (str): Second word.\n",
    "\n",
    "    Returns:\n",
    "        float: Maximum path similarity between word1 and word2.\n",
    "    \"\"\"\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "\n",
    "    # Consider only pairs with matching POS, and take the maximum similarity\n",
    "    max_sim = 0.0\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            if s1.pos() == s2.pos():\n",
    "                sim = s1.path_similarity(s2)\n",
    "                if sim is not None and sim > max_sim:\n",
    "                    max_sim = sim\n",
    "    return max_sim\n",
    "\n",
    "def harmonic_mean(x, y):\n",
    "    \"\"\"\n",
    "    Compute the harmonic mean of two numbers.\n",
    "\n",
    "    Parameters:\n",
    "        x (float): First number.\n",
    "        y (float): Second number.\n",
    "\n",
    "    Returns:\n",
    "        float: The harmonic mean of the two numbers.\n",
    "    \"\"\"\n",
    "    if (x + y) > 0:\n",
    "        return 2 * x * y / (x + y)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>longest_common_substring</th>\n",
       "      <th>longest_common_subsequence</th>\n",
       "      <th>greedy_string_tiling</th>\n",
       "      <th>2_gram_char</th>\n",
       "      <th>3_gram_char</th>\n",
       "      <th>4_gram_char</th>\n",
       "      <th>1_gram_word_Jaccard</th>\n",
       "      <th>3_gram_word_Jaccard</th>\n",
       "      <th>4_gram_word_Jaccard</th>\n",
       "      <th>2_gram_word_Jaccard_without_SW</th>\n",
       "      <th>4_gram_word_Jaccard_without_SW</th>\n",
       "      <th>1_gram_word_Containment_without_SW_a</th>\n",
       "      <th>2_gram_word_Containment_without_SW_a</th>\n",
       "      <th>1_gram_word_Containment_without_SW_b</th>\n",
       "      <th>2_gram_word_Containment_without_SW_b</th>\n",
       "      <th>average_similarity</th>\n",
       "      <th>lexical_substitution_system</th>\n",
       "      <th>wordnet_augmented_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>97</td>\n",
       "      <td>69</td>\n",
       "      <td>0.797310</td>\n",
       "      <td>0.722290</td>\n",
       "      <td>0.632530</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.633658</td>\n",
       "      <td>0.772088</td>\n",
       "      <td>0.773645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>49</td>\n",
       "      <td>24</td>\n",
       "      <td>0.666997</td>\n",
       "      <td>0.515727</td>\n",
       "      <td>0.461880</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.780363</td>\n",
       "      <td>0.521026</td>\n",
       "      <td>0.659982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>52</td>\n",
       "      <td>25</td>\n",
       "      <td>0.700097</td>\n",
       "      <td>0.547782</td>\n",
       "      <td>0.434828</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.574525</td>\n",
       "      <td>0.799408</td>\n",
       "      <td>0.619091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "      <td>127</td>\n",
       "      <td>124</td>\n",
       "      <td>0.926470</td>\n",
       "      <td>0.852009</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.805629</td>\n",
       "      <td>0.808532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>0.597739</td>\n",
       "      <td>0.367355</td>\n",
       "      <td>0.282144</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.547487</td>\n",
       "      <td>0.462910</td>\n",
       "      <td>0.492864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  longest_common_substring  longest_common_subsequence  \\\n",
       "0           0                        69                          97   \n",
       "1           1                        24                          49   \n",
       "2           2                        25                          52   \n",
       "3           3                       124                         127   \n",
       "4           4                        20                          57   \n",
       "\n",
       "   greedy_string_tiling  2_gram_char  3_gram_char  4_gram_char  \\\n",
       "0                    69     0.797310     0.722290     0.632530   \n",
       "1                    24     0.666997     0.515727     0.461880   \n",
       "2                    25     0.700097     0.547782     0.434828   \n",
       "3                   124     0.926470     0.852009     0.827556   \n",
       "4                    20     0.597739     0.367355     0.282144   \n",
       "\n",
       "   1_gram_word_Jaccard  3_gram_word_Jaccard  4_gram_word_Jaccard  \\\n",
       "0             0.533333             0.342857             0.323529   \n",
       "1             0.388889             0.047619             0.000000   \n",
       "2             0.333333             0.074074             0.038462   \n",
       "3             0.607143             0.576923             0.560000   \n",
       "4             0.192308             0.000000             0.000000   \n",
       "\n",
       "   2_gram_word_Jaccard_without_SW  4_gram_word_Jaccard_without_SW  \\\n",
       "0                        0.444444                        0.375000   \n",
       "1                        0.307692                        0.181818   \n",
       "2                        0.058824                        0.000000   \n",
       "3                        0.555556                        0.500000   \n",
       "4                        0.050000                        0.000000   \n",
       "\n",
       "   1_gram_word_Containment_without_SW_a  2_gram_word_Containment_without_SW_a  \\\n",
       "0                              0.562500                              0.533333   \n",
       "1                              0.857143                              0.666667   \n",
       "2                              0.555556                              0.125000   \n",
       "3                              0.916667                              0.909091   \n",
       "4                              0.250000                              0.090909   \n",
       "\n",
       "   1_gram_word_Containment_without_SW_b  2_gram_word_Containment_without_SW_b  \\\n",
       "0                              0.750000                              0.727273   \n",
       "1                              0.500000                              0.363636   \n",
       "2                              0.454545                              0.100000   \n",
       "3                              0.611111                              0.588235   \n",
       "4                              0.272727                              0.100000   \n",
       "\n",
       "   average_similarity  lexical_substitution_system  wordnet_augmented_overlap  \n",
       "0            0.633658                     0.772088                   0.773645  \n",
       "1            0.780363                     0.521026                   0.659982  \n",
       "2            0.574525                     0.799408                   0.619091  \n",
       "3            0.827160                     0.805629                   0.808532  \n",
       "4            0.547487                     0.462910                   0.492864  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Feature Visualization**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
