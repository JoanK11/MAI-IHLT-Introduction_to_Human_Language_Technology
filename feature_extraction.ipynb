{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the necessary libraries\n",
    "%pip install pandas\n",
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\joanc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\joanc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\joanc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\joanc\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Import the required libraries\n",
    "\n",
    "# Standard Libraries\n",
    "import os\n",
    "\n",
    "# Data manipulation and analysis\n",
    "import pandas as pd\n",
    "\n",
    "# Natural Language Processing\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "# Machine Learning\n",
    "import joblib\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('wordnet')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stopwords = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **1. Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **2. Data Loading**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we load the training and test datasets along with their corresponding gold standard similarity scores from various corpora. We begin by installing and importing the necessary libraries and packages. Next, we define a function to load and process the datasets, and we transform them into structured DataFrames for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training samples: 2234\n",
      "Number of test samples: 3108\n"
     ]
    }
   ],
   "source": [
    "from functions.load_dataset import load_dataset\n",
    "\n",
    "data_dir = 'datasets'\n",
    "\n",
    "# Load Training Data\n",
    "train_data = load_dataset(data_dir, dataset_category='train')\n",
    "columns = ['sentence_0', 'sentence_1', 'score', 'dataset_name']\n",
    "train_data = pd.DataFrame(train_data, columns=columns)\n",
    "\n",
    "# Load Test Data\n",
    "test_data = load_dataset(data_dir, dataset_category='test')\n",
    "columns = ['sentence_0', 'sentence_1', 'score', 'dataset_name']\n",
    "test_data = pd.DataFrame(test_data, columns=columns)\n",
    "\n",
    "# Display the number of samples in each dataset\n",
    "print(f\"Number of training samples: {len(train_data)}\")\n",
    "print(f\"Number of test samples: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **3. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the data is loaded, we apply preprocessing steps to prepare the datasets for feature extraction. Specifically, we:\n",
    "\n",
    "1. Tokenize Sentences: Use NLTK's `word_tokenize` to split sentences into individual tokens.\n",
    "2. Remove Punctuation: Eliminate punctuation tokens to retain only meaningful words.\n",
    "3. Replace Contractions: Replace contractions based on a custom dictionary, inspired by TakenLab's paper. \n",
    "4. Lemmatize Words: Apply NLTK's `WordNetLemmatizer` with Part-of-Speech (POS) tagging to reduce words to their base forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tokenize Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tokens_0'] = train_data['sentence_0'].apply(nltk.word_tokenize)\n",
    "train_data['tokens_1'] = train_data['sentence_1'].apply(nltk.word_tokenize)\n",
    "\n",
    "test_data['tokens_0'] = test_data['sentence_0'].apply(nltk.word_tokenize)\n",
    "test_data['tokens_1'] = test_data['sentence_1'].apply(nltk.word_tokenize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data['tokens_0'] = train_data['tokens_0'].apply(lambda tokens: [word for word in tokens if any(char.isalnum() for char in word)])\n",
    "train_data['tokens_1'] = train_data['tokens_1'].apply(lambda tokens: [word for word in tokens if any(char.isalnum() for char in word)])\n",
    "\n",
    "test_data['tokens_0'] = test_data['tokens_0'].apply(lambda tokens: [word for word in tokens if any(char.isalnum() for char in word)])\n",
    "test_data['tokens_1'] = test_data['tokens_1'].apply(lambda tokens: [word for word in tokens if any(char.isalnum() for char in word)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace Contractions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.preprocessing import replace_contractions\n",
    "\n",
    "train_data['tokens_0'] = train_data['tokens_0'].apply(replace_contractions)\n",
    "train_data['tokens_1'] = train_data['tokens_1'].apply(replace_contractions)\n",
    "\n",
    "test_data['tokens_0'] = test_data['tokens_0'].apply(replace_contractions)\n",
    "test_data['tokens_1'] = test_data['tokens_1'].apply(replace_contractions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lemmatize Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.preprocessing import lemmatize\n",
    "\n",
    "# Obtain Lemmatized Words\n",
    "train_data['lemmas_0'] = train_data.apply(lambda row: lemmatize(row, \"tokens_0\", True), axis=1)\n",
    "train_data['lemmas_1'] = train_data.apply(lambda row: lemmatize(row, \"tokens_1\", True), axis=1)\n",
    "\n",
    "test_data['lemmas_0'] = test_data.apply(lambda row: lemmatize(row, \"tokens_0\", True), axis=1)\n",
    "test_data['lemmas_1'] = test_data.apply(lambda row: lemmatize(row, \"tokens_1\", True), axis=1)\n",
    "\n",
    "# Join Lemmatized Words\n",
    "train_data['sentence_lemmas_0'] = train_data.apply(lambda row: \" \".join(row[\"lemmas_0\"]), axis=1)\n",
    "train_data['sentence_lemmas_1'] = train_data.apply(lambda row: \" \".join(row[\"lemmas_1\"]), axis=1)\n",
    "\n",
    "test_data['sentence_lemmas_0'] = test_data.apply(lambda row: \" \".join(row[\"lemmas_0\"]), axis=1)\n",
    "test_data['sentence_lemmas_1'] = test_data.apply(lambda row: \" \".join(row[\"lemmas_1\"]), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save and Load Preprocessed Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Training and Test Data\n",
    "train_data.to_csv('datasets/train_preprocessed.csv')\n",
    "test_data.to_csv('datasets/test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training and Test Data\n",
    "train_data = pd.read_csv('datasets/train_preprocessed.csv')\n",
    "test_data = pd.read_csv('datasets/test_preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **4. Feature Extraction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **5. Feature Computation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functions.features import harmonic_mean, P_WN\n",
    "\n",
    "def compute_features(data):\n",
    "    \"\"\"\n",
    "    Computes a comprehensive set of similarity features for each pair of sentences in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing 'sentence_lemmas_0' and 'sentence_lemmas_1' columns.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with computed similarity features.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame()\n",
    "\n",
    "    # Similarity Features\n",
    "    features['longest_common_substring'] = data.apply(lambda row: longest_common_substring(row[\"sentence_lemmas_0\"], row[\"sentence_lemmas_1\"]), axis=1)\n",
    "    features['longest_common_subsequence'] = data.apply(lambda row: longest_common_subsequence(row[\"sentence_lemmas_0\"], row[\"sentence_lemmas_1\"]), axis=1)\n",
    "    features['greedy_string_tiling'] = data.apply(lambda row: optimized_gst(row[\"sentence_lemmas_0\"], row[\"sentence_lemmas_1\"], min_match_length=1), axis=1)\n",
    "\n",
    "    # Character n-gram Similarity Features\n",
    "    features['2_gram_char'] = data.apply(lambda row: similarity_char_ngrams(row[\"lemmas_0\"], row[\"lemmas_1\"], 2), axis=1)\n",
    "    features['3_gram_char'] = data.apply(lambda row: similarity_char_ngrams(row[\"lemmas_0\"], row[\"lemmas_1\"], 3), axis=1)\n",
    "    features['4_gram_char'] = data.apply(lambda row: similarity_char_ngrams(row[\"lemmas_0\"], row[\"lemmas_1\"], 4), axis=1)\n",
    "\n",
    "    # Word n-gram Jaccard Similarity Features\n",
    "    features['1_gram_word_Jaccard'] = data.apply(lambda row: similarity_words_ngrams_jaccard(row[\"lemmas_0\"], row[\"lemmas_1\"], 1), axis=1)\n",
    "    features['3_gram_word_Jaccard'] = data.apply(lambda row: similarity_words_ngrams_jaccard(row[\"lemmas_0\"], row[\"lemmas_1\"], 3), axis=1)\n",
    "    features['4_gram_word_Jaccard'] = data.apply(lambda row: similarity_words_ngrams_jaccard(row[\"lemmas_0\"], row[\"lemmas_1\"], 4), axis=1)\n",
    "\n",
    "    # Word n-gram Jaccard Similarity Features without Stopwords\n",
    "    features['2_gram_word_Jaccard_without_SW'] = data.apply(lambda row: similarity_words_ngrams_jaccard(row[\"lemmas_0\"], row[\"lemmas_1\"], 2, use_stopwords=True), axis=1)\n",
    "    features['4_gram_word_Jaccard_without_SW'] = data.apply(lambda row: similarity_words_ngrams_jaccard(row[\"lemmas_0\"], row[\"lemmas_1\"], 4, use_stopwords=True), axis=1)\n",
    "\n",
    "    # Word n-gram Containment Similarity Features without Stopwords (a)\n",
    "    features['1_gram_word_Containment_without_SW_a'] = data.apply(lambda row: similarity_words_ngrams_containment(row[\"lemmas_0\"], row[\"lemmas_1\"], 1, use_stopwords=True), axis=1)\n",
    "    features['2_gram_word_Containment_without_SW_a'] = data.apply(lambda row: similarity_words_ngrams_containment(row[\"lemmas_0\"], row[\"lemmas_1\"], 2, use_stopwords=True), axis=1)\n",
    "\n",
    "    # Word n-gram Containment Similarity Features without Stopwords (b)\n",
    "    features['1_gram_word_Containment_without_SW_b'] = data.apply(lambda row: similarity_words_ngrams_containment(row[\"lemmas_1\"], row[\"lemmas_0\"], 1, use_stopwords=True), axis=1)\n",
    "    features['2_gram_word_Containment_without_SW_b'] = data.apply(lambda row: similarity_words_ngrams_containment(row[\"lemmas_1\"], row[\"lemmas_0\"], 2, use_stopwords=True), axis=1)\n",
    "\n",
    "    features['average_similarity'] = data.apply(lambda row: average_similarity(row[\"lemmas_0\"], row[\"lemmas_1\"]), axis=1)\n",
    "\n",
    "    # Lexical Substitution System Feature\n",
    "    data['lemmas_with_disambiguation_0'] = data.apply(lambda row: tokens_to_synsets_name(row[\"lemmas_0\"]), axis=1)\n",
    "    data['lemmas_with_disambiguation_1'] = data.apply(lambda row: tokens_to_synsets_name(row[\"lemmas_1\"]), axis=1)\n",
    "    features['lexical_substitution_system'] = data.apply(lambda row: similarity_lemmas(row['lemmas_with_disambiguation_0'], row['lemmas_with_disambiguation_1']), axis=1)\n",
    "\n",
    "\n",
    "    # WordNet-Augmented Word Overlap (TakeLab)\n",
    "    features['wordnet_augmented_overlap'] = data.apply(lambda row: harmonic_mean(\n",
    "                P_WN([w for w in row[\"lemmas_0\"] if w not in stopwords], [w for w in row[\"lemmas_1\"] if w not in stopwords]),\n",
    "                P_WN([w for w in row[\"lemmas_1\"] if w not in stopwords], [w for w in row[\"lemmas_0\"] if w not in stopwords])\n",
    "            ), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_train = ...\n",
    "features_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Training Features to a CSV File\n",
    "features_train.to_csv('features/features_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save Test Features to a CSV File\n",
    "features_test.to_csv('features/features_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Training and Test Features\n",
    "features_train = pd.read_csv('features/features_train.csv')\n",
    "features_test  = pd.read_csv('features/features_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m harmonic_mean, P_WN\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Add the new feature using the wordnet_augmented_word_overlap function\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m features_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet_augmented_overlap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mtest_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mharmonic_mean\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                \u001b[49m\u001b[43mP_WN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlemmas_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlemmas_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m                \u001b[49m\u001b[43mP_WN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlemmas_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlemmas_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joanc\\miniconda3\\Lib\\site-packages\\pandas\\core\\frame.py:10374\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[1;34m(self, func, axis, raw, result_type, args, by_row, engine, engine_kwargs, **kwargs)\u001b[0m\n\u001b[0;32m  10360\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mapply\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m frame_apply\n\u001b[0;32m  10362\u001b[0m op \u001b[38;5;241m=\u001b[39m frame_apply(\n\u001b[0;32m  10363\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m  10364\u001b[0m     func\u001b[38;5;241m=\u001b[39mfunc,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m  10372\u001b[0m     kwargs\u001b[38;5;241m=\u001b[39mkwargs,\n\u001b[0;32m  10373\u001b[0m )\n\u001b[1;32m> 10374\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapply\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joanc\\miniconda3\\Lib\\site-packages\\pandas\\core\\apply.py:916\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    913\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mraw:\n\u001b[0;32m    914\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_raw(engine\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine, engine_kwargs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine_kwargs)\n\u001b[1;32m--> 916\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\joanc\\miniconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1063\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1061\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply_standard\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m   1062\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m-> 1063\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_series_generator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1064\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1065\u001b[0m         results, res_index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_series_numba()\n",
      "File \u001b[1;32mc:\\Users\\joanc\\miniconda3\\Lib\\site-packages\\pandas\\core\\apply.py:1081\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1078\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m option_context(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmode.chained_assignment\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m   1079\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(series_gen):\n\u001b[0;32m   1080\u001b[0m         \u001b[38;5;66;03m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[1;32m-> 1081\u001b[0m         results[i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[0;32m   1083\u001b[0m             \u001b[38;5;66;03m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m             \u001b[38;5;66;03m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m             results[i] \u001b[38;5;241m=\u001b[39m results[i]\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mfunctions\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfeatures\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m harmonic_mean, P_WN\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# Add the new feature using the wordnet_augmented_word_overlap function\u001b[39;00m\n\u001b[0;32m      4\u001b[0m features_test[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwordnet_augmented_overlap\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m test_data\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m row: harmonic_mean(\n\u001b[0;32m      5\u001b[0m                 P_WN([w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmas_0\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords], [w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmas_1\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords]),\n\u001b[1;32m----> 6\u001b[0m                 \u001b[43mP_WN\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlemmas_1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlemmas_0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mw\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m             ), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[0;32m      8\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\joanc\\Desktop\\MAI-IHLT-Introduction_to_Human_Language_Technology\\functions\\features.py:183\u001b[0m, in \u001b[0;36mP_WN\u001b[1;34m(S1, S2)\u001b[0m\n\u001b[0;32m    180\u001b[0m         score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;66;03m# Find the best similarity if exact match is not found\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m         best_sim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmax\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwordnet_path_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mword2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mS2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    184\u001b[0m         score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m best_sim\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(S1)\n",
      "File \u001b[1;32mc:\\Users\\joanc\\Desktop\\MAI-IHLT-Introduction_to_Human_Language_Technology\\functions\\features.py:183\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    180\u001b[0m         score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    181\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    182\u001b[0m         \u001b[38;5;66;03m# Find the best similarity if exact match is not found\u001b[39;00m\n\u001b[1;32m--> 183\u001b[0m         best_sim \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m((\u001b[43mwordnet_path_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mword1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword2\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m word2 \u001b[38;5;129;01min\u001b[39;00m S2), default\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.0\u001b[39m)\n\u001b[0;32m    184\u001b[0m         score \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m best_sim\n\u001b[0;32m    186\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m score \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(S1)\n",
      "File \u001b[1;32mc:\\Users\\joanc\\Desktop\\MAI-IHLT-Introduction_to_Human_Language_Technology\\functions\\features.py:208\u001b[0m, in \u001b[0;36mwordnet_path_similarity\u001b[1;34m(word1, word2)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m s2 \u001b[38;5;129;01min\u001b[39;00m synsets2:\n\u001b[0;32m    207\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m s1\u001b[38;5;241m.\u001b[39mpos() \u001b[38;5;241m==\u001b[39m s2\u001b[38;5;241m.\u001b[39mpos():\n\u001b[1;32m--> 208\u001b[0m         sim \u001b[38;5;241m=\u001b[39m \u001b[43ms1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    209\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m sim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m sim \u001b[38;5;241m>\u001b[39m max_sim:\n\u001b[0;32m    210\u001b[0m             max_sim \u001b[38;5;241m=\u001b[39m sim\n",
      "File \u001b[1;32mc:\\Users\\joanc\\miniconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:871\u001b[0m, in \u001b[0;36mSynset.path_similarity\u001b[1;34m(self, other, verbose, simulate_root)\u001b[0m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpath_similarity\u001b[39m(\u001b[38;5;28mself\u001b[39m, other, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, simulate_root\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m    843\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    844\u001b[0m \u001b[38;5;124;03m    Path Distance Similarity:\u001b[39;00m\n\u001b[0;32m    845\u001b[0m \u001b[38;5;124;03m    Return a score denoting how similar two word senses are, based on the\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[38;5;124;03m        itself.\u001b[39;00m\n\u001b[0;32m    867\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    869\u001b[0m     distance \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshortest_path_distance(\n\u001b[0;32m    870\u001b[0m         other,\n\u001b[1;32m--> 871\u001b[0m         simulate_root\u001b[38;5;241m=\u001b[39msimulate_root \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_root() \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mother\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_needs_root\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m),\n\u001b[0;32m    872\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m distance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m distance \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    874\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\joanc\\miniconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:479\u001b[0m, in \u001b[0;36mSynset._needs_root\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    478\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_needs_root\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 479\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pos \u001b[38;5;241m==\u001b[39m NOUN \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wordnet_corpus_reader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_version\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m1.6\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    480\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    481\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\joanc\\miniconda3\\Lib\\site-packages\\nltk\\corpus\\reader\\wordnet.py:1456\u001b[0m, in \u001b[0;36mWordNetCorpusReader.get_version\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1454\u001b[0m fh\u001b[38;5;241m.\u001b[39mseek(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m   1455\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m fh:\n\u001b[1;32m-> 1456\u001b[0m     match \u001b[38;5;241m=\u001b[39m \u001b[43mre\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mWord[nN]et (\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+|\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43md+) Copyright\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mline\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1457\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m match \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1458\u001b[0m         version \u001b[38;5;241m=\u001b[39m match\u001b[38;5;241m.\u001b[39mgroup(\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\joanc\\miniconda3\\Lib\\re\\__init__.py:174\u001b[0m, in \u001b[0;36msearch\u001b[1;34m(pattern, string, flags)\u001b[0m\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Try to apply the pattern to all of the string, returning\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m    172\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39mfullmatch(string)\n\u001b[1;32m--> 174\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msearch\u001b[39m(pattern, string, flags\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    175\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[0;32m    176\u001b[0m \u001b[38;5;124;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[0;32m    177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(pattern, flags)\u001b[38;5;241m.\u001b[39msearch(string)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from functions.features import harmonic_mean, P_WN\n",
    "\n",
    "# Add the new feature using the wordnet_augmented_word_overlap function\n",
    "features_test['wordnet_augmented_overlap'] = test_data.apply(lambda row: harmonic_mean(\n",
    "                P_WN([w for w in row[\"lemmas_0\"] if w not in stopwords], [w for w in row[\"lemmas_1\"] if w not in stopwords]),\n",
    "                P_WN([w for w in row[\"lemmas_1\"] if w not in stopwords], [w for w in row[\"lemmas_0\"] if w not in stopwords])\n",
    "            ), axis=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def P_WN(S1, S2):\n",
    "    \"\"\"\n",
    "    Compute P_WN(S1, S2) metric as described in the TakeLab paper.\n",
    "\n",
    "    Parameters:\n",
    "        S1 (list): List of tokenized words from the first sentence.\n",
    "        S2 (list): List of tokenized words from the second sentence.\n",
    "\n",
    "    Returns:\n",
    "        float: The computed P_WN(S1, S2) value.\n",
    "    \"\"\"\n",
    "    if len(S1) == 0:\n",
    "        return 0.0\n",
    "\n",
    "    score = 0.0\n",
    "    S2_set = set(S2) # Slight optimization for membership checks\n",
    "    for word1 in S1:\n",
    "        if word1 in S2_set:\n",
    "            score += 1.0\n",
    "        else:\n",
    "            # Find the best similarity if exact match is not found\n",
    "            best_sim = max((wordnet_path_similarity(word1, word2) for word2 in S2), default=0.0)\n",
    "            score += best_sim\n",
    "\n",
    "    return score / len(S1)\n",
    "\n",
    "def wordnet_path_similarity(word1, word2):\n",
    "    \"\"\"\n",
    "    Compute the maximum WordNet path similarity between all synset pairs of two given words.\n",
    "    Only consider synsets that share the same part-of-speech (POS).\n",
    "\n",
    "    Parameters:\n",
    "        word1 (str): First word.\n",
    "        word2 (str): Second word.\n",
    "\n",
    "    Returns:\n",
    "        float: Maximum path similarity between word1 and word2.\n",
    "    \"\"\"\n",
    "    synsets1 = wn.synsets(word1)\n",
    "    synsets2 = wn.synsets(word2)\n",
    "\n",
    "    # Consider only pairs with matching POS, and take the maximum similarity\n",
    "    max_sim = 0.0\n",
    "    for s1 in synsets1:\n",
    "        for s2 in synsets2:\n",
    "            if s1.pos() == s2.pos():\n",
    "                sim = s1.path_similarity(s2)\n",
    "                if sim is not None and sim > max_sim:\n",
    "                    max_sim = sim\n",
    "    return max_sim\n",
    "\n",
    "def harmonic_mean(x, y):\n",
    "    \"\"\"\n",
    "    Compute the harmonic mean of two numbers.\n",
    "\n",
    "    Parameters:\n",
    "        x (float): First number.\n",
    "        y (float): Second number.\n",
    "\n",
    "    Returns:\n",
    "        float: The harmonic mean of the two numbers.\n",
    "    \"\"\"\n",
    "    if (x + y) > 0:\n",
    "        return 2 * x * y / (x + y)\n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>longest_common_substring</th>\n",
       "      <th>longest_common_subsequence</th>\n",
       "      <th>greedy_string_tiling</th>\n",
       "      <th>2_gram_char</th>\n",
       "      <th>3_gram_char</th>\n",
       "      <th>4_gram_char</th>\n",
       "      <th>1_gram_word_Jaccard</th>\n",
       "      <th>3_gram_word_Jaccard</th>\n",
       "      <th>4_gram_word_Jaccard</th>\n",
       "      <th>2_gram_word_Jaccard_without_SW</th>\n",
       "      <th>4_gram_word_Jaccard_without_SW</th>\n",
       "      <th>1_gram_word_Containment_without_SW_a</th>\n",
       "      <th>2_gram_word_Containment_without_SW_a</th>\n",
       "      <th>1_gram_word_Containment_without_SW_b</th>\n",
       "      <th>2_gram_word_Containment_without_SW_b</th>\n",
       "      <th>average_similarity</th>\n",
       "      <th>lexical_substitution_system</th>\n",
       "      <th>wordnet_augmented_overlap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>69</td>\n",
       "      <td>97</td>\n",
       "      <td>69</td>\n",
       "      <td>0.797310</td>\n",
       "      <td>0.722290</td>\n",
       "      <td>0.632530</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.342857</td>\n",
       "      <td>0.323529</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.375000</td>\n",
       "      <td>0.562500</td>\n",
       "      <td>0.533333</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.633658</td>\n",
       "      <td>0.772088</td>\n",
       "      <td>0.773645</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>24</td>\n",
       "      <td>49</td>\n",
       "      <td>24</td>\n",
       "      <td>0.666997</td>\n",
       "      <td>0.515727</td>\n",
       "      <td>0.461880</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.307692</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.857143</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.363636</td>\n",
       "      <td>0.780363</td>\n",
       "      <td>0.521026</td>\n",
       "      <td>0.659982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>52</td>\n",
       "      <td>25</td>\n",
       "      <td>0.700097</td>\n",
       "      <td>0.547782</td>\n",
       "      <td>0.434828</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.074074</td>\n",
       "      <td>0.038462</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.454545</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.574525</td>\n",
       "      <td>0.799408</td>\n",
       "      <td>0.619091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>124</td>\n",
       "      <td>127</td>\n",
       "      <td>124</td>\n",
       "      <td>0.926470</td>\n",
       "      <td>0.852009</td>\n",
       "      <td>0.827556</td>\n",
       "      <td>0.607143</td>\n",
       "      <td>0.576923</td>\n",
       "      <td>0.560000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.916667</td>\n",
       "      <td>0.909091</td>\n",
       "      <td>0.611111</td>\n",
       "      <td>0.588235</td>\n",
       "      <td>0.827160</td>\n",
       "      <td>0.805629</td>\n",
       "      <td>0.808532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>20</td>\n",
       "      <td>57</td>\n",
       "      <td>20</td>\n",
       "      <td>0.597739</td>\n",
       "      <td>0.367355</td>\n",
       "      <td>0.282144</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>0.547487</td>\n",
       "      <td>0.462910</td>\n",
       "      <td>0.492864</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  longest_common_substring  longest_common_subsequence  \\\n",
       "0           0                        69                          97   \n",
       "1           1                        24                          49   \n",
       "2           2                        25                          52   \n",
       "3           3                       124                         127   \n",
       "4           4                        20                          57   \n",
       "\n",
       "   greedy_string_tiling  2_gram_char  3_gram_char  4_gram_char  \\\n",
       "0                    69     0.797310     0.722290     0.632530   \n",
       "1                    24     0.666997     0.515727     0.461880   \n",
       "2                    25     0.700097     0.547782     0.434828   \n",
       "3                   124     0.926470     0.852009     0.827556   \n",
       "4                    20     0.597739     0.367355     0.282144   \n",
       "\n",
       "   1_gram_word_Jaccard  3_gram_word_Jaccard  4_gram_word_Jaccard  \\\n",
       "0             0.533333             0.342857             0.323529   \n",
       "1             0.388889             0.047619             0.000000   \n",
       "2             0.333333             0.074074             0.038462   \n",
       "3             0.607143             0.576923             0.560000   \n",
       "4             0.192308             0.000000             0.000000   \n",
       "\n",
       "   2_gram_word_Jaccard_without_SW  4_gram_word_Jaccard_without_SW  \\\n",
       "0                        0.444444                        0.375000   \n",
       "1                        0.307692                        0.181818   \n",
       "2                        0.058824                        0.000000   \n",
       "3                        0.555556                        0.500000   \n",
       "4                        0.050000                        0.000000   \n",
       "\n",
       "   1_gram_word_Containment_without_SW_a  2_gram_word_Containment_without_SW_a  \\\n",
       "0                              0.562500                              0.533333   \n",
       "1                              0.857143                              0.666667   \n",
       "2                              0.555556                              0.125000   \n",
       "3                              0.916667                              0.909091   \n",
       "4                              0.250000                              0.090909   \n",
       "\n",
       "   1_gram_word_Containment_without_SW_b  2_gram_word_Containment_without_SW_b  \\\n",
       "0                              0.750000                              0.727273   \n",
       "1                              0.500000                              0.363636   \n",
       "2                              0.454545                              0.100000   \n",
       "3                              0.611111                              0.588235   \n",
       "4                              0.272727                              0.100000   \n",
       "\n",
       "   average_similarity  lexical_substitution_system  wordnet_augmented_overlap  \n",
       "0            0.633658                     0.772088                   0.773645  \n",
       "1            0.780363                     0.521026                   0.659982  \n",
       "2            0.574525                     0.799408                   0.619091  \n",
       "3            0.827160                     0.805629                   0.808532  \n",
       "4            0.547487                     0.462910                   0.492864  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **6. Feature Visualization**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
